
<!DOCTYPE html>

<html class="no-js" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<link href="../../assets/images/favicon.png" rel="icon"/>
<meta content="mkdocs-1.4.0, mkdocs-material-8.5.6" name="generator"/>
<title>Lecture notes - obsidian-mkdocs template</title>
<link href="../../assets/stylesheets/main.20d9efc8.min.css" rel="stylesheet"/>
<link href="../../assets/stylesheets/palette.cbb835fc.min.css" rel="stylesheet"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&amp;display=fallback" rel="stylesheet"/>
<style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
<script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
</head>
<body data-md-color-accent="indigo" data-md-color-primary="pink" data-md-color-scheme="default" dir="ltr">
<script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
<input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/>
<input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/>
<label class="md-overlay" for="__drawer"></label>
<div data-md-component="skip">
<a class="md-skip" href="#strong-convexity">
          Skip to content
        </a>
</div>
<div data-md-component="announce">
</div>
<header class="md-header" data-md-component="header">
<nav aria-label="Header" class="md-header__inner md-grid">
<a aria-label="obsidian-mkdocs template" class="md-header__button md-logo" data-md-component="logo" href="../.." title="obsidian-mkdocs template">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"></path></svg>
</a>
<label class="md-header__button md-icon" for="__drawer">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"></path></svg>
</label>
<div class="md-header__title" data-md-component="header-title">
<div class="md-header__ellipsis">
<div class="md-header__topic">
<span class="md-ellipsis">
            obsidian-mkdocs template
          </span>
</div>
<div class="md-header__topic" data-md-component="header-topic">
<span class="md-ellipsis">
            
              Lecture notes
            
          </span>
</div>
</div>
</div>
<form class="md-header__option" data-md-component="palette">
<input aria-label="Switch to dark mode" class="md-option" data-md-color-accent="indigo" data-md-color-media="(prefers-color-scheme: light)" data-md-color-primary="pink" data-md-color-scheme="default" id="__palette_1" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_2" hidden="" title="Switch to dark mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"></path></svg>
</label>
<input aria-label="Switch to light mode" class="md-option" data-md-color-accent="blue" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-primary="pink" data-md-color-scheme="slate" id="__palette_2" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_1" hidden="" title="Switch to light mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"></path></svg>
</label>
</form>
<label class="md-header__button md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"></path></svg>
</label>
<div class="md-search" data-md-component="search" role="dialog">
<label class="md-search__overlay" for="__search"></label>
<div class="md-search__inner" role="search">
<form class="md-search__form" name="search">
<input aria-label="Search" autocapitalize="off" autocomplete="off" autocorrect="off" class="md-search__input" data-md-component="search-query" name="query" placeholder="Search" required="" spellcheck="false" type="text"/>
<label class="md-search__icon md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"></path></svg>
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"></path></svg>
</label>
<nav aria-label="Search" class="md-search__options">
<button aria-label="Clear" class="md-search__icon md-icon" tabindex="-1" title="Clear" type="reset">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"></path></svg>
</button>
</nav>
</form>
<div class="md-search__output">
<div class="md-search__scrollwrap" data-md-scrollfix="">
<div class="md-search-result" data-md-component="search-result">
<div class="md-search-result__meta">
            Initializing search
          </div>
<ol class="md-search-result__list"></ol>
</div>
</div>
</div>
</div>
</div>
</nav>
</header>
<div class="md-container" data-md-component="container">
<main class="md-main" data-md-component="main">
<div class="md-main__inner md-grid">
<div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Navigation" class="md-nav md-nav--primary" data-md-level="0">
<label class="md-nav__title" for="__drawer">
<a aria-label="obsidian-mkdocs template" class="md-nav__button md-logo" data-md-component="logo" href="../.." title="obsidian-mkdocs template">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"></path></svg>
</a>
    obsidian-mkdocs template
  </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../..">
        Obsidian Notes
      </a>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" id="__nav_2" type="checkbox"/>
<label class="md-nav__link" for="__nav_2">
          Features
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-label="Features" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_2">
<span class="md-nav__icon md-icon"></span>
          Features
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../Features/LaTeX%20Math%20Support/">
        LaTeX Math Support
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../Features/Mermaid%20Diagrams/">
        Mermaid diagrams
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../Features/Text%20Formatting/">
        Text Formatting
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" id="__nav_3" type="checkbox"/>
<label class="md-nav__link" for="__nav_3">
          Opts
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-label="Opts" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_3">
<span class="md-nav__icon md-icon"></span>
          Opts
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--active">
<input class="md-nav__toggle md-toggle" data-md-toggle="toc" id="__toc" type="checkbox"/>
<label class="md-nav__link md-nav__link--active" for="__toc">
          Lecture notes
          <span class="md-nav__icon md-icon"></span>
</label>
<a class="md-nav__link md-nav__link--active" href="./">
        Lecture notes
      </a>
<nav aria-label="Table of contents" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#strong-convexity">
    Strong Convexity
  </a>
<nav aria-label="Strong Convexity" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#characteristics">
    Characteristics
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#gains-from-this">
    Gains from this
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#lecture-8-convex-optimizations-contd150922">
    Lecture 8 - Convex optimizations contd.(15/09/22)
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#lecture-9-convex-optimizations-contd">
    Lecture 9 - Convex optimizations contd.
  </a>
<nav aria-label="Lecture 9 - Convex optimizations contd." class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#convex-sets">
    Convex sets
  </a>
<nav aria-label="Convex sets" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#properties-of-convex-sets">
    Properties of Convex sets
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#projected-gradient-descent">
    Projected Gradient Descent
  </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#ch4-frank-wolfe-conditional-gradient-method">
    CH4: Frank Wolfe Conditional Gradient Method
  </a>
<nav aria-label="CH4: Frank Wolfe Conditional Gradient Method" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#frank-wolfe-algorithm">
    Frank Wolfe algorithm
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#lecture-10-frank-wolfe-contd">
    Lecture 10: Frank Wolfe contd.
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#lecture-11-frank-wolfe-method-contd-and-power-iteration">
    Lecture 11: Frank Wolfe method contd. and power-iteration
  </a>
<nav aria-label="Lecture 11: Frank Wolfe method contd. and power-iteration" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#power-iteration-method">
    Power iteration method
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#lecture-12-ch5-beyond-gradients">
    Lecture 12 : Ch5 - Beyond gradients
  </a>
<nav aria-label="Lecture 12 : Ch5 - Beyond gradients" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#newtons-method">
    Newton's method
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#chapter-13-bfgs-quasi-newtons-method">
    Chapter 13: BFGS (Quasi newtons method)
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#chapter-14-accelerated-methods">
    Chapter 14: Accelerated methods
  </a>
<nav aria-label="Chapter 14: Accelerated methods" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#momentum-acceleration">
    Momentum Acceleration
  </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" id="__nav_4" type="checkbox"/>
<label class="md-nav__link" for="__nav_4">
          Topic 1
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-label="Topic 1" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_4">
<span class="md-nav__icon md-icon"></span>
          Topic 1
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../Topic%201/Note%201/">
        Note 1
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../Topic%201/Note%202/">
        Note 2
      </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Table of contents" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#strong-convexity">
    Strong Convexity
  </a>
<nav aria-label="Strong Convexity" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#characteristics">
    Characteristics
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#gains-from-this">
    Gains from this
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#lecture-8-convex-optimizations-contd150922">
    Lecture 8 - Convex optimizations contd.(15/09/22)
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#lecture-9-convex-optimizations-contd">
    Lecture 9 - Convex optimizations contd.
  </a>
<nav aria-label="Lecture 9 - Convex optimizations contd." class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#convex-sets">
    Convex sets
  </a>
<nav aria-label="Convex sets" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#properties-of-convex-sets">
    Properties of Convex sets
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#projected-gradient-descent">
    Projected Gradient Descent
  </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#ch4-frank-wolfe-conditional-gradient-method">
    CH4: Frank Wolfe Conditional Gradient Method
  </a>
<nav aria-label="CH4: Frank Wolfe Conditional Gradient Method" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#frank-wolfe-algorithm">
    Frank Wolfe algorithm
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#lecture-10-frank-wolfe-contd">
    Lecture 10: Frank Wolfe contd.
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#lecture-11-frank-wolfe-method-contd-and-power-iteration">
    Lecture 11: Frank Wolfe method contd. and power-iteration
  </a>
<nav aria-label="Lecture 11: Frank Wolfe method contd. and power-iteration" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#power-iteration-method">
    Power iteration method
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#lecture-12-ch5-beyond-gradients">
    Lecture 12 : Ch5 - Beyond gradients
  </a>
<nav aria-label="Lecture 12 : Ch5 - Beyond gradients" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#newtons-method">
    Newton's method
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#chapter-13-bfgs-quasi-newtons-method">
    Chapter 13: BFGS (Quasi newtons method)
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#chapter-14-accelerated-methods">
    Chapter 14: Accelerated methods
  </a>
<nav aria-label="Chapter 14: Accelerated methods" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#momentum-acceleration">
    Momentum Acceleration
  </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-content" data-md-component="content">
<article class="md-content__inner md-typeset">
<p>min f(x): || <span class="arithmatex">\(\nabla\)</span>f(<span class="arithmatex">\(x_1\)</span>) - <span class="arithmatex">\(\nabla\)</span>f(<span class="arithmatex">\(x_2\)</span>)||<span class="arithmatex">\(_2\)</span> <span class="arithmatex">\(\leq\)</span> L . ||<span class="arithmatex">\(x_1 - x_2\)</span>||
<span class="arithmatex">\(x_{t+1} = x_t - \eta \nabla f(x_t)\)</span>
f(x) is convex
<span class="arithmatex">\(min_t || \nabla f(x_t)||_2 \leq O(\frac{1}{\sqrt t})\)</span>
<span class="arithmatex">\(||x_{t+1} - x*||_2^2 =^{GD} ||x_t - \eta \nabla f(x+t) - x*||_2^2\)</span>
                        = <span class="arithmatex">\(||x_t - x*||_2^2 - 2\eta &lt;\nabla f(x_t), x_t - x*&gt;\)</span>
From L-Lipschitz: <span class="arithmatex">\(\frac{1}{L} || \nabla f(x_1) - \nabla f(x_2)||_2 \leq &lt;\nabla f(x_1) - \nabla f(x_2), x* - x_t&gt;\)</span>
At x1=x* and x2=xt
<span class="arithmatex">\(\frac{1}{L} ||\nabla f(x_2)||_2^2 \leq &lt;-\nabla f(x_t), x* - x_t&gt;\)</span>
<span class="arithmatex">\(&lt;\nabla f(x_t), x* - x_t&gt; \leq \frac{1}{L} ||\nabla f(x_2)||_2^2\)</span>
<span class="arithmatex">\(||x_{t+1} - x*||_2^2 \leq || x_t - x*||_2^2 + \eta^2 || \nabla f(x_t)||_2^2 - \frac{2\eta}{L} ||\nabla f(x_t)||_2^2\)</span>
<span class="arithmatex">\(=|| x_t - x*||_2^2 - \eta (\frac{2}{L} - \eta) ||\nabla f(x_t)||_2^2\)</span>
This shows that <span class="arithmatex">\(f(x_{t+1}) \leq f(x_t))\)</span> which is similar to non-convex optimizations
which only tells us that it always climbs down from the positive values
* From Lipschitz, 
<span class="arithmatex">\(f(x_t) \leq f(x*) - \eta (\frac{2}{L} - \eta) ||\nabla f(x_t)||_2^2\)</span>
* From Convex optimization as seen above,</p>
<p><span class="arithmatex">\(f(x_t) - f(x*) \leq &lt;\nabla f(x_t), x_t - x*&gt;\)</span>
From Cauchy Scwartz,
<span class="arithmatex">\(f(x_t) - f(x*) \leq ||x_t - x*||_2 . ||\nabla(x_t)||_2\)</span></p>
<p>Combining both,
<span class="arithmatex">\(f(x_{t+1}) - f(x*) \leq f(x_t) - f(x*) - \eta (\frac{2}{L} - \eta) \frac{f(x_t) - f(x*)^2}{||x_t - x*||_2^2}\)</span>
With <span class="arithmatex">\(\eta = \frac{1}{L}\)</span></p>
<p><span class="arithmatex">\(f(x_t) - f(x*) \leq \frac{2L(f(x_0)-f(x*).||x_0-x*||_2^2)}{2L||x_0 - x*||_2^2 + \textbf{T} (f(x_0) - f(x*))}\)</span> = O(1/T)
<span class="arithmatex">\(\implies\)</span> O(1/T) making it a faster convergence than non-convex optimization</p>
<h3 id="strong-convexity">Strong Convexity<a class="headerlink" href="#strong-convexity" title="Permanent link">¶</a></h3>
<p>A strong convexity means that the curve is <strong>always</strong> steep enough to make enough progress, ie, we can lower bound to a quadratic function</p>
<h5 id="characteristics">Characteristics<a class="headerlink" href="#characteristics" title="Permanent link">¶</a></h5>
<ul>
<li><span class="arithmatex">\(\nabla^2 f(x) \succeq \mu I\)</span><ul>
<li>Lowerbound</li>
</ul>
</li>
</ul>
<h3 id="gains-from-this">Gains from this<a class="headerlink" href="#gains-from-this" title="Permanent link">¶</a></h3>
<p><span class="arithmatex">\(min f(x):\)</span> L - Smooth, 
                <span class="arithmatex">\(\mu\)</span> - Strongly convex
<span class="arithmatex">\(||x_{t+1} - x*||_2^2 \leq || x_t - x*||_2^2 + \eta^2 || \nabla f(x_t)||_2^2 - \frac{2\eta}{L} ||\nabla f(x_t)||_2^2\)</span>
<span class="arithmatex">\(&lt;\nabla f(x) - \nabla f(y)&gt; \geq \frac{\mu L}{\mu + L} ||x - y||_2^2 + \frac{1}{\mu + L} ||\nabla f(x) - \nabla f(y)||_2^2\)</span>
at y = x* and x = x_t
<span class="arithmatex">\(&lt;-\nabla f)x_t), x* - x_t&gt; \geq \frac{\mu L}{\mu + L} ||x_t - x*||_2^2 + \frac{1}{\mu + L} ||\nabla f(x)_t||_2^2\)</span>
<span class="arithmatex">\(||x_{t+1} 0 x*||_2^2 \geq .. (1 - \frac{2\mu \eta L}{\mu +L})||x_t - x*||_2^2\)</span>
if  <span class="arithmatex">\(\eta \leq \frac{2}{\mu + L}\)</span>
<span class="arithmatex">\(\leq (1 - \frac{2\mu \eta L}{\mu +L})||x_t - x*||_2^2\)</span>
    = O(<span class="arithmatex">\(log \frac{1}{\epsilon}\)</span>)</p>
<h1 id="lecture-8-convex-optimizations-contd150922">Lecture 8 - Convex optimizations contd.(15/09/22)<a class="headerlink" href="#lecture-8-convex-optimizations-contd150922" title="Permanent link">¶</a></h1>
<ul>
<li>Gradient descent has an upper bound of <span class="arithmatex">\(O(\frac{1}{T})\)</span> and a lower bound of <span class="arithmatex">\(O(\frac{1}{T^2})\)</span></li>
<li><span class="arithmatex">\(||x_t - x*||_2^2 \leq (\frac{k-1}{k+1})^T ||x_0 - x*||_2^2\)</span>  such that <span class="arithmatex">\(\kappa=\frac{L}{\mu}\)</span> <ul>
<li>L -&gt; Smoothness</li>
<li><span class="arithmatex">\(\mu\)</span> -&gt; Strongly convex</li>
</ul>
</li>
<li>minimizing f(x) brings a upper bound of O(<span class="arithmatex">\(\kappa log \frac{1}{\epsilon}\)</span>) vs a Lower bound of O(<span class="arithmatex">\(\sqrt{\kappa}log\frac{1}{\epsilon}\)</span>)</li>
<li>PL inequality with Lipschitz aids gradient descent with non-convex optimizations</li>
<li>We can also project a point to the nearest convex set (Projecting gradient descent at each time step to maintain the constraints with the problems)</li>
</ul>
<h1 id="lecture-9-convex-optimizations-contd">Lecture 9 - Convex optimizations contd.<a class="headerlink" href="#lecture-9-convex-optimizations-contd" title="Permanent link">¶</a></h1>
<h3 id="convex-sets">Convex sets<a class="headerlink" href="#convex-sets" title="Permanent link">¶</a></h3>
<p>All points in the set follows <span class="arithmatex">\(\alpha x + (1-\alpha) y \in C\)</span>
<span class="arithmatex">\(\Pi_C(x) = arg min_{y \in C} \parallel x - y \parallel_2^2\)</span>
    Euclidean distance is used in general for the projection </p>
<h4 id="properties-of-convex-sets">Properties of Convex sets<a class="headerlink" href="#properties-of-convex-sets" title="Permanent link">¶</a></h4>
<p><span class="arithmatex">\(\parallel x - \Pi_C(x) \parallel_2^2 \leq \parallel x - y \parallel_2^2\)</span>
<span class="arithmatex">\(&lt;\Pi_C(x) - y, \Pi_C(x) - x&gt; \leq 0\)</span>
<span class="arithmatex">\(\parallel \Pi_C(y) - \Pi_C(x) \parallel_2^2 \leq \parallel x - y \parallel_2^2\)</span></p>
<h4 id="projected-gradient-descent">Projected Gradient Descent<a class="headerlink" href="#projected-gradient-descent" title="Permanent link">¶</a></h4>
<p><span class="arithmatex">\(x_{t+1} = \Pi_C(x_t - \eta \nabla f(x_t))\)</span>
Step 1: <span class="arithmatex">\(x' = x_t - \eta \nabla f(x_t)\)</span>
Step 2: <span class="arithmatex">\(x_{t+1} = \Pi_C(x')\)</span></p>
<p><strong>DEMO TIME</strong></p>
<h1 id="ch4-frank-wolfe-conditional-gradient-method">CH4: Frank Wolfe Conditional Gradient Method<a class="headerlink" href="#ch4-frank-wolfe-conditional-gradient-method" title="Permanent link">¶</a></h1>
<p><span class="arithmatex">\(min_{x \in C}f(x)\)</span>: L Smoothness</p>
<ol>
<li><span class="arithmatex">\(f(x) \leq f(x_t) + &lt;\nabla f(x_t), x - x_t&gt; + \frac{L}{2} \parallel x - x_t \parallel_2^2\)</span></li>
<li><span class="arithmatex">\(min_{x \in C} [f(x_t) + &lt;\nabla f(x_t), x - x_t&gt; + \frac{L}{2} \parallel x - x_t \parallel_2^2]\)</span>
<span class="arithmatex">\(min_{x \in C} [\frac{1}{L}\parallel \nabla f(x_t) \parallel_2^2 + &lt;\nabla f(x_t), x - x_t&gt; + \frac{L}{2} \parallel x - x_t \parallel_2^2]\)</span> (Since gradient at xt is independent of x)
    <span class="arithmatex">\(min_{x \in C} [\frac{L}{2} \parallel x - (x_t - \frac{1}{L} \nabla f(x_t))\parallel_2^2]\)</span>
<span class="arithmatex">\(min_{x \in C} [ \parallel x - (x_t - \frac{1}{L} \nabla f(x_t))\parallel_2^2]\)</span>
<span class="arithmatex">\(min_{x \in C} \parallel x - Y\parallel_2^2\)</span> Where Y = <span class="arithmatex">\((x_t - \frac{1}{L} \nabla f(x_t))\)</span></li>
</ol>
<h4 id="frank-wolfe-algorithm">Frank Wolfe algorithm<a class="headerlink" href="#frank-wolfe-algorithm" title="Permanent link">¶</a></h4>
<ol>
<li><span class="arithmatex">\(f(x) = f(x_t) + &lt;\nabla f(x_t), x - x_t&gt;\)</span><ol>
<li>We can use this since its constrained by convexity and L-Smoothness</li>
</ol>
</li>
<li><span class="arithmatex">\(min_{x \in C} [f(x_t) + &lt;\nabla f(x_t), x - x_t&gt;]\)</span></li>
<li><span class="arithmatex">\(min_{x \in C} [&lt;\nabla f(x_t), x - x_t&gt;]\)</span> -&gt; Answer is <span class="arithmatex">\(s_t\)</span><ol>
<li>This leads to the corners of the set which are the sparse solutions</li>
</ol>
</li>
</ol>
<h1 id="lecture-10-frank-wolfe-contd">Lecture 10: Frank Wolfe contd.<a class="headerlink" href="#lecture-10-frank-wolfe-contd" title="Permanent link">¶</a></h1>
<ol>
<li><span class="arithmatex">\(x_{t+1} = x_t + \eta d_t = x_t + \eta_t(s_t - x_t)\)</span><ol>
<li><span class="arithmatex">\(= (1 - \eta_t)x_t + \eta_t . s_t\)</span></li>
</ol>
</li>
<li><span class="arithmatex">\(\eta_t = \frac{2}{t+2}\)</span></li>
<li>f: C -&gt; R, L-Smooth, <span class="arithmatex">\(\exists x* \in C\)</span>. Then f satisfies:<ol>
<li><span class="arithmatex">\(f(x_{t+1}) - f(x*) \leq \frac{2 LD^2}{t+2} = O(\frac{1}{t})\)</span> </li>
<li>where <span class="arithmatex">\(\eta_t = \frac{2}{t+2}\)</span></li>
<li>D = <span class="arithmatex">\(max_{x, y in C} \parallel x - y \parallel_2\)</span> (Diameter of C)<ol>
<li>This projection works efficiently per iteration than the projected gradient descent</li>
<li>O(1/t) vs O(1/<span class="arithmatex">\(\sqrt t\)</span>)</li>
<li>Q. Assume that <span class="arithmatex">\(C = {x \in R: \parallel x \parallel_1 \leq \lambda}\)</span><ol>
<li>FW: <ol>
<li><span class="arithmatex">\(s_t = min {&lt;\nabla f(x_t), x&gt;} ST \parallel x \parallel_1 \leq \lambda\)</span></li>
<li><span class="arithmatex">\(x_{t+1} = (1 - \eta_t)x_t + \eta_t . s_t\)</span> where <span class="arithmatex">\(\eta_t = \frac{2}{t+2}\)</span></li>
</ol>
</li>
<li>VERSUS Projected Gradient Descent<ol>
<li><span class="arithmatex">\(x'_{t+1} = x_t -\eta \nabla f(X_t)\)</span></li>
<li><span class="arithmatex">\(x_{t+1} = min \parallel x - x'_{t+1} \parallel_2^2 ST \parallel x \parallel \leq \lambda\)</span><ol>
<li>O(plogp) for the projection step(2)</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li><span class="arithmatex">\(s_t = min {&lt;\nabla f(x_t), x&gt;} ST \parallel x \parallel_1 \leq \lambda\)</span><ol>
<li>Without constraint <span class="arithmatex">\(&lt;\nabla f(x_t), x&gt;\)</span></li>
<li>Minimize it as much as possible<ol>
<li>Go in the opposite direction as much as we can =&gt;  &lt;., .&gt; -&gt; - <span class="arithmatex">\(\infty\)</span></li>
<li><a, x=""> = <span class="arithmatex">\(\sum a_i x_i\)</span> with <span class="arithmatex">\(\parallel x \parallel_1 \leq \lambda\)</span> <ol>
<li>Which minimizes to <span class="arithmatex">\(x_i\)</span> = -<span class="arithmatex">\(\lambda\)</span><ol>
<li><span class="arithmatex">\(s_t = -\lambda sign(&lt;\nabla f(x_t), e_i*&gt;).e_i*\)</span></li>
</ol>
</li>
<li>We will traverse the vector to find the max value here</li>
<li>Makes it O(p) for the projection step</li>
</ol>
</a,></li>
</ol>
</li>
</ol>
</li>
<li><span class="arithmatex">\(min_{x \in R^{p*p}} f(x)\)</span> ST <span class="arithmatex">\(\parallel X \parallel_* \leq 1\)</span><ol>
<li><span class="arithmatex">\(\parallel X \parallel_* = \sum \sigma_i(x)\)</span></li>
<li>A low rank matrix appears<ol>
<li>Projected gradient descent:<ol>
<li><span class="arithmatex">\(x'_{t+1} = x_t -\eta \nabla f(X_t)\)</span></li>
<li><span class="arithmatex">\(x_{t+1} = min \parallel x - x'_{t+1} \parallel_F^2 ST \parallel x \parallel_* \leq 1\)</span><ol>
<li><span class="arithmatex">\(O(p^3)\)</span> step making it very expensive</li>
</ol>
</li>
</ol>
</li>
<li>Frank Wolfe:<ol>
<li><span class="arithmatex">\(s_t = min {&lt;\nabla f(x_t), x&gt;} ST \parallel x \parallel_* \leq 1\)</span></li>
<li><span class="arithmatex">\(x_{t+1} = (1 - \eta_t)x_t + \eta_t . s_t\)</span> where <span class="arithmatex">\(\eta_t = \frac{2}{t+2}\)</span></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<h1 id="lecture-11-frank-wolfe-method-contd-and-power-iteration">Lecture 11: Frank Wolfe method contd. and power-iteration<a class="headerlink" href="#lecture-11-frank-wolfe-method-contd-and-power-iteration" title="Permanent link">¶</a></h1>
<ol>
<li><span class="arithmatex">\(s_t = -1 . u_1 v_1^T\)</span> where u and v are left and right singular vectors of <span class="arithmatex">\(\nabla f(x_t)\)</span></li>
<li>This shows that the second step jus adds a rank 1 matrix per iteration.<ol>
<li>Makes it a <span class="arithmatex">\(O(p^2)\)</span></li>
</ol>
</li>
<li><em>The frank-wolfe algorithm performs worse than projected gradient descent when dealing with non-convex optimizations.</em></li>
</ol>
<h3 id="power-iteration-method">Power iteration method<a class="headerlink" href="#power-iteration-method" title="Permanent link">¶</a></h3>
<p>Let <span class="arithmatex">\(A \in R^{p*p}\)</span> real, symmetric matrix
<span class="arithmatex">\(q \in R^p\)</span> is the variable to find the largest eigenvector
<span class="arithmatex">\(A = U \Lambda U^T\)</span> (Eigenvalue decomposition)
<span class="arithmatex">\(diag(\Lambda) = (\lambda_1, \lambda_2, ..., \lambda_p)\ where\ |\lambda_1| \geq |\lambda_2| \geq ... \geq |\lambda_p|\)</span>
POWER ITERATION:
    1. <span class="arithmatex">\(q_{t+1} = \frac{A q_t}{\parallel Aq_t \parallel_2}\)</span>
        1. <span class="arithmatex">\(q_{t+1} = \frac{A^t q_0}{\parallel A^t q_0 \parallel_2}\)</span>
    2. max <span class="arithmatex">\(q^T A q\)</span>
        1. <span class="arithmatex">\(\nabla = Aq\)</span>
            1. We keep calculating the gradient and using that in coming steps
        2. ST <span class="arithmatex">\(\parallel q \parallel_2 = 1\)</span>
    3. Equal complexity compared to gradient descent
    4. Non-convex operation but still converges in linear time O(<span class="arithmatex">\(log(\frac{1}{\epsilon})\)</span>)
        1. Per iteration time complexity of O(<span class="arithmatex">\(p^2\)</span>)
    5. For getting all eigenvalues, we can run this p times making it <span class="arithmatex">\(O(p^3)\)</span>
    6. If q is orthogonal to the required eigenvector, the algorithm never converges(but never happens in random init)</p>
<h1 id="lecture-12-ch5-beyond-gradients">Lecture 12 : Ch5 - Beyond gradients<a class="headerlink" href="#lecture-12-ch5-beyond-gradients" title="Permanent link">¶</a></h1>
<h3 id="newtons-method">Newton's method<a class="headerlink" href="#newtons-method" title="Permanent link">¶</a></h3>
<ol>
<li>From Taylors series expansion in second order:<ol>
<li><span class="arithmatex">\(f(x + \Delta x) = f(x) + &lt;\nabla f(x), \Delta x&gt; + \frac{1}{2} &lt;\nabla^2 f(x) \Delta x, \Delta x&gt;\)</span><ol>
<li><span class="arithmatex">\(f(x + \nabla x) \leq f(x) + &lt;\nabla f(x), \nabla x&gt; + \frac{L}{2} \parallel (\nabla x)\parallel_2^2\)</span> <ol>
<li>From lipschitz where <span class="arithmatex">\(\nabla^2 \leq LI\)</span></li>
</ol>
</li>
<li>We can decide to compute the hessian instead of using lipschitz as an upper bound</li>
</ol>
</li>
<li>If we equate derivatives to zero:<ol>
<li><span class="arithmatex">\(\nabla_{\Delta x} f(x + \Delta x) = 0 \implies \Delta x = -(\nabla^2 f(x))^{-2} \nabla f(x)\)</span></li>
<li><span class="arithmatex">\(H_t = \nabla^2 f(x_t)\)</span></li>
</ol>
</li>
<li>For an iteration, by newtons method:<ol>
<li><span class="arithmatex">\(x_{t+1} = x_t - \eta H_t^{-1}\nabla f(x_t)\)</span><ol>
<li>H is not invertible if the matrix is positive semi-definite as the determinant of Hessian becomes 0 at the points where eigenvalues defines as 0</li>
</ol>
</li>
<li>Example: <span class="arithmatex">\(min \frac{1}{2} \parallel b - Ax \parallel_2^2, A \in R^{pxp}\)</span><ol>
<li><span class="arithmatex">\(\nabla^2 f(x) = A^TA\)</span></li>
<li><span class="arithmatex">\(x_{t+1} = x_t - \eta (A^TA)^{-1} (-A^T(b - Ax_t))\)</span></li>
<li><span class="arithmatex">\(x_{t+1} = x_t + \eta (A^TA)^{-1}(A^TAx* - A^TAx_t) = x_t + \eta(x* - x_t)\)</span><ol>
<li>If <span class="arithmatex">\(\eta = 1\)</span></li>
<li><span class="arithmatex">\(x_{t+1} = x*\)</span><ol>
<li>Which means that it can converge in a single step</li>
</ol>
</li>
<li>If we are close enough (<span class="arithmatex">\(\parallel x_0 - x* \parallel_2 &lt; \frac{2\mu}{3M}\)</span> where <span class="arithmatex">\(\nabla^2f(x*) \succcurlyeq \mu I\)</span> and <span class="arithmatex">\(\parallel \nabla^2 f(x*) - \nabla f(x) \parallel_2 \leq M \parallel x - y \parallel_2\)</span>)<ol>
<li>Newtons method converges quadratically with O(log log(<span class="arithmatex">\(\frac{1}{\epsilon}\)</span>)) from the equation <span class="arithmatex">\(\parallel x_{t+1} - x* \parallel_2 \leq \frac{M \parallel x_t - x* \parallel_2^2}{2(\mu - M\parallel x_t - x* \parallel_2)}\)</span><ol>
<li>Due to the norm squares compared to just norm</li>
</ol>
</li>
<li>But before it is close enough, it starts sublinear</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<h1 id="chapter-13-bfgs-quasi-newtons-method">Chapter 13: BFGS (Quasi newtons method)<a class="headerlink" href="#chapter-13-bfgs-quasi-newtons-method" title="Permanent link">¶</a></h1>
<ol>
<li>Approximate <span class="arithmatex">\(H_{t+1}\)</span> to <span class="arithmatex">\(g_{t+1}\)</span> such that<ol>
<li><span class="arithmatex">\(\nabla g_{t+1}(0) = \nabla f(x_{t+1})\)</span></li>
<li><span class="arithmatex">\(\nabla g_{t+1}(-\Delta x) = \nabla f(x_t)\)</span></li>
</ol>
</li>
<li>From 2. and Taylors expansion,<ol>
<li><span class="arithmatex">\(H_{t+1} \Delta x = \nabla f(x_{t+1}) - \nabla f(x_t)\)</span>[ Secant method ]</li>
<li><span class="arithmatex">\(\Delta x^T H_{t+1} \Delta x = \Delta x^T (\nabla f(x_{t+1}) - \nabla f(x_t)) \succ 0\)</span> if H is positive definite (Assumption from convex)</li>
</ol>
</li>
<li>BFGS states<ol>
<li><span class="arithmatex">\(min_{B \succ 0} \parallel B - B_t \parallel_F^2\)</span></li>
<li>ST. <span class="arithmatex">\(B = B^T\)</span></li>
<li><span class="arithmatex">\(\Delta x = B(\nabla f(x_{t+1}) - \nabla f(x_t))\)</span><ol>
<li>Has a closed form solution</li>
<li><span class="arithmatex">\(B_{t+1} = (I - \frac{s_t y_t^T}{s_t^T y_t}) B_t (I - \frac{y_t s_t^T}{s_t^T y_t}) + \frac{s_t s_t^T}{s_t^Ty_t}\)</span></li>
<li>Where <span class="arithmatex">\(y_t = \nabla f(x_{t+1}) - \nabla f(x_t)\)</span>, <span class="arithmatex">\(s_t = \Delta x\)</span></li>
</ol>
</li>
</ol>
</li>
<li>SR-1 method works on Rank 1 instead of rank 2<ol>
<li><span class="arithmatex">\(H_{t+1} = H_t + \sigma v v^T\)</span> where <span class="arithmatex">\(\sigma = +- 1\)</span> (and secant eq satisfied)</li>
<li>SR1 shows that<ol>
<li><span class="arithmatex">\(B_t+1 = B_t + \frac{(s_t - B_t y_t)(s_t - B_ty_t)^T}{(s_t - B_ty_t)^T y_t}\)</span></li>
<li>Useful for indefinite Hessian approximations in non-convex (BFGS needs convex)</li>
<li>Failed neural network training tho :(</li>
</ol>
</li>
</ol>
</li>
</ol>
<p>For strongly convex problems
1. GD: <span class="arithmatex">\(\parallel x_{k+1} - x* \parallel_2 \leq c \parallel x_k - x* \parallel\)</span> , 0&lt;c&lt;1   -&gt; Linear
2. Newtons: <span class="arithmatex">\(\parallel x_{k+1} - x* \parallel_2 \leq c \parallel x_k - x* \parallel^2\)</span> , 0&lt;c&lt;1  -&gt; Quadratic
3. BFGS: <span class="arithmatex">\(\parallel x_{k+1} - x* \parallel_2 \leq c_k \parallel x_k - x* \parallel\)</span> , <span class="arithmatex">\(c_k \implies 0\)</span>   -&gt; Super-Linear
    1. ck decreases over iteration but not as fast as square in newtons
4. ExtraGradient(<span class="arithmatex">\(min_x max_y f(x, y)\)</span>): <span class="arithmatex">\(X_{t+1} = x_t - \eta \nabla f(x_t - \gamma \nabla f(x_t))\)</span> (Out of syllabus)
    1. Faster than GD in this scenario (GD diverges in minmax) and constant diff with minimization problems</p>
<p>What to do when not even gradient is available?
Bisection method, genetic agorithms, simulated annealing, etc</p>
<p>Finite differences method is used instead
Mainly used in adversarial training</p>
<h1 id="chapter-14-accelerated-methods">Chapter 14: Accelerated methods<a class="headerlink" href="#chapter-14-accelerated-methods" title="Permanent link">¶</a></h1>
<h3 id="momentum-acceleration">Momentum Acceleration<a class="headerlink" href="#momentum-acceleration" title="Permanent link">¶</a></h3>
<p>We move based on
1. <span class="arithmatex">\(x_{t+1} = x_t - \eta \nabla f(x_t) + \beta (x_t - x_{t-1})\)</span>
, including a momentum factor
This has same time complexity as gradient descent but doubles the space it takes.</p>
<p>When we take L-smooth and <span class="arithmatex">\(\mu\)</span> convex,
$$\begin{bmatrix}
x_{t+1} - x<em> \
x_t - x</em> 
\end{bmatrix} = 
\begin{bmatrix} 
x_t - \eta \nabla f(x_t) + \beta (x_t - x_{t-1}) - x<em> \
x_t - x</em>
\end{bmatrix} $$<span class="arithmatex">\(<span class="arithmatex">\(= 
\begin{bmatrix}
x_t + \beta(x_t - x_{t-1} - x*) \\
x_t - x*
\end{bmatrix} - \eta
\begin{bmatrix}
\nabla f(x_t) \\
0
\end{bmatrix}\)</span>\)</span>= $$\parallel\begin{bmatrix}
x_{t+1} - x<em> \
x_t - x</em> 
\end{bmatrix}\parallel_2 = 
\parallel \begin{bmatrix}
(1+\beta)I &amp; -\beta I \
I &amp; 0
\end{bmatrix} \begin{bmatrix}
x_t - x<em> \
x_{t-1} - x</em>
\end{bmatrix} - \eta \begin{bmatrix}
\nabla^2 f(x_t)(x_t - x<em>) \
0
\end{bmatrix} \parallel_2
$$
$$
\parallel\begin{bmatrix}
x_{t+1} - x</em> \
x_t - x<em> 
\end{bmatrix}\parallel_2 = 
\parallel \begin{bmatrix}
(1+\beta)I - \eta \nabla^2 f(x_t) &amp; -\beta I \
I &amp; 0
\end{bmatrix} \begin{bmatrix}
x_t - x</em> \
x_{t-1} - x*
\end{bmatrix} \parallel_2
$$</p>
<p>Applying cauchy schwartz to this:</p>
<p>$$</p>
<p>\parallel\begin{bmatrix}
x_{t+1} - x<em> \
x_t - x</em> 
\end{bmatrix}\parallel_2  \leq  
\parallel \begin{bmatrix}
(1+\beta)I - \eta \nabla^2 f(x_t) &amp; -\beta I \
I &amp; 0
\end{bmatrix} \parallel_2  .\parallel \begin{bmatrix}
x_t - x<em> \
x_{t-1} - x</em>
\end{bmatrix} \parallel_2
$$
Where the first matrix becomes the spectrum of a matrix (it depends on the eigenvalues)
<span class="arithmatex">\(\phi(\nabla^2f(.), \beta, \eta)\)</span></p>
<p><span class="arithmatex">\(\nabla^2 f(.) &gt; 0, \nabla^2f(.)=U \Lambda U^T\)</span>
$$ 
\parallel \begin{bmatrix}
(1+\beta)I - \eta U \Lambda U^T &amp; -\beta I \
I &amp; 0
\end{bmatrix} \parallel_2 = \parallel \begin{bmatrix}
U^T &amp; 0 \
0 &amp; U^T
\end{bmatrix}
\begin{bmatrix}
... \ ...
\end{bmatrix}
\begin{bmatrix}
U &amp; 0 \
0 &amp; U
\end{bmatrix} \parallel_2</p>
<p>$$
$$
= \parallel \begin{bmatrix}
(1 + \beta)I - \eta \Lambda &amp; -\beta I \
I &amp; 0
\end{bmatrix} \parallel_2 = max_i \parallel \begin{bmatrix}
1 + \beta - \eta \lambda_i &amp; -\beta \
1 &amp; 0
\end{bmatrix} \parallel_2
$$
Since everything in LHS is diagonal</p>
<p><span class="arithmatex">\(\leq max{|1 - \sqrt{\eta \mu} | , |1 - \sqrt{\eta L}}|\)</span></p>
<p>To solve,
<span class="arithmatex">\(\xi^2 - (1 + \\beta - \mu \lambda_i)\xi + \beta = 0\)</span></p>
<p><span class="arithmatex">\(\eta = \frac{4}{(\sqrt{\mu} + \sqrt{L})^2}, \beta = \frac{\sqrt{L} - \sqrt{\mu}}{\sqrt{L} + \sqrt{\mu}}\)</span>
<span class="arithmatex">\(\lambda \in [\mu, L]\)</span></p>
<div class="arithmatex">\[
\parallel\begin{bmatrix}
x_{t+1} - x* \\
x_t - x* 
\end{bmatrix}\parallel_2  = \leq max{|1 - \sqrt{\eta \mu} | , |1 - \sqrt{\eta L}}| \begin{bmatrix}
x_t - x* \\
x_{t-1} - x*
\end{bmatrix}
\]</div>
<div class="arithmatex">\[
\parallel\begin{bmatrix}
x_{t+1} - x* \\
x_t - x* 
\end{bmatrix}\parallel_2  = \leq (\frac{\sqrt{\kappa} - 1}{\sqrt{\kappa} + 1 })\begin{bmatrix}
x_t - x* \\
x_{t-1} - x*
\end{bmatrix}
\]</div>
</article>
</div>
</div>
</main>
<footer class="md-footer">
<nav aria-label="Footer" class="md-footer__inner md-grid">
<a aria-label="Previous: Text Formatting" class="md-footer__link md-footer__link--prev" href="../../Features/Text%20Formatting/" rel="prev">
<div class="md-footer__button md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"></path></svg>
</div>
<div class="md-footer__title">
<div class="md-ellipsis">
<span class="md-footer__direction">
                Previous
              </span>
              Text Formatting
            </div>
</div>
</a>
<a aria-label="Next: Note 1" class="md-footer__link md-footer__link--next" href="../../Topic%201/Note%201/" rel="next">
<div class="md-footer__title">
<div class="md-ellipsis">
<span class="md-footer__direction">
                Next
              </span>
              Note 1
            </div>
</div>
<div class="md-footer__button md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"></path></svg>
</div>
</a>
</nav>
<div class="md-footer-meta md-typeset">
<div class="md-footer-meta__inner md-grid">
<div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" rel="noopener" target="_blank">
      Material for MkDocs
    </a>
</div>
</div>
</div>
</footer>
</div>
<div class="md-dialog" data-md-component="dialog">
<div class="md-dialog__inner md-typeset"></div>
</div>
<script id="__config" type="application/json">{"base": "../..", "features": [], "search": "../../assets/javascripts/workers/search.5bf1dace.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
<script src="../../assets/javascripts/bundle.078830c0.min.js"></script>
<script src="../../javascripts/mathjax.js"></script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</body>
</html>